> **[info]**  
paper: http://aclweb.org/anthology/N/N16/N16-1030.pdf  
code of the LSTM-CRF:https://github.com/glample/tagger  
code of the Stack-LSTM: https://github.com/clab/stack-lstm-ner  

Neural Architectures for Named Entity Recognition

> **[success]**   
Name Entity Recognition， NER，命名实体识别问题  
https://blog.csdn.net/suoyan1539/article/details/79550889




# 输入向量x是什么来的？  

每个单词的向量化表示  
难点：为NER生成独立的表示很难  

设计原理：  
1. 基于orthographic(拼写的)、morphological（形态学的）的分析，所以：  
使用与单词拼写有关的表达，即字符级表示。  
2. 虽然name本身会变化，但appear in regular context，所以：  
使用与单词order有关的表示
3. dropout

## 字符级的embedding

优势：  
形态学rich的语言  
处理out of vocabulary问题  
dependency parsing  

字符级embedding也是用的双向LSTM，然后两边的结果拼到一起。区别是每个LSTM都是在一个单词结束时才输出。  
再加上单词级的embedding，来自查表和UNK embedding  

为什么用双向LSTM？  
因为RNN/LSTM受最近的点影响特别大。双向分别用于识别单词的前缀和后缀。此处不适合用CNN，因为前缀和后缀的位置固定，不具有平移不变性。  

单词级的look-up table使用pretrained代替随机初始化，性能有明显提升。  
skip-n-gram + fine-tuned    

字符级表示 + 单词级表示 + pretrained = 效果一般  
字符级表示 + 单词级表示 + pretrained + dropout = 效果一般  


# 训练  

BP + online + SGD + gradient clipping