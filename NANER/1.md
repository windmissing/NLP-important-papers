> **[info]**  
paper: http://aclweb.org/anthology/N/N16/N16-1030.pdf  
code of the LSTM-CRF:https://github.com/glample/tagger  
code of the Stack-LSTM: https://github.com/clab/stack-lstm-ner  

Neural Architectures for Named Entity Recognition

> **[success]**   
Name Entity Recognition， NER，命名实体识别问题  
https://blog.csdn.net/suoyan1539/article/details/79550889


# Transition-Based Chunking Model

> **[success]**   
transition-based dependency parsing 基于转移的依赖分析

As an alternative to the LSTM-CRF discussed in the previous section, we explore a new architecture that chunks and labels a sequence of inputs using an algorithm similar to transition-based dependency parsing. This model directly constructs representations of the multi-token names (e.g., the name Mark Watney is composed into a single representation).

This model relies on a stack data structure to incrementally construct chunks of the input. To obtain representations of this stack used for predicting subsequent actions, we use the Stack-LSTM presented by Dyer et al. (2015), in which the LSTM is augmented with a “stack pointer.” While sequential LSTMs model sequences from left to right, stack LSTMs permit embedding of a stack of objects that are both added to (using a push operation) and removed from (using a pop operation). This allows the Stack-LSTM to work like a stack that maintains a “summary embedding” of its contents. We refer to this model as Stack-LSTM or S-LSTM model for simplicity.

> **[success]**   
stack - LSTM = LSTM + 栈顶指针    
普通LSTM： 从左往右依次加数据  
栈LSTM： 栈顶可以增加或移除数据，栈中的信息可以看作是内容的summary embedding  
[?] arc-standard parser  

Finally, we refer interested readers to the original paper (Dyer et al., 2015) for details about the Stack-LSTM model since in this paper we merely use the same architecture through a new transition-based algorithm presented in the following Section.

## Chunking Algorithm

We designed a transition inventory which is given in Figure 2 that is inspired by transition-based parsers, in particular the arc-standard parser of Nivre (2004). In this algorithm, we make use of two stacks (designated output and stack representing, respectively, completed chunks and scratch space) and a buffer that contains the words that have yet to be processed. The transition inventory contains the following transitions: The SHIFT transition moves a word from the buffer to the stack, the OUT transition moves a word from the buffer directly into the output stack while the REDUCE (y) transition pops all items from the top of the stack creating a “chunk,” labels this with label y, and pushes a representation of this chunk onto the output stack. The algorithm com-
pletes when the stack and buffer are both empty. The algorithm is depicted in Figure 2, which shows the sequence of operations required to process the sentence Mark Watney visited Mars.

The model is parameterized by defining a probability distribution over actions at each time step, given the current contents of the stack, buffer, and output, as well as the history of actions taken. Following Dyer et al. (2015), we use stack LSTMs to compute a fixed dimensional embedding of each of these, and take a concatenation of these to obtain the full algorithm state. This representation is used to define a distribution over the possible actions that can be taken at each time step. The model is trained to maximize the conditional probability of
sequences of reference actions (extracted from a la-
beled training corpus) given the input sentences. To
label a new input sequence at test time, the maxi-
mum probability action is chosen greedily until the
algorithm reaches a termination state. Although this
is not guaranteed to find a global optimum, it is ef-
fective in practice. Since each token is either moved
directly to the output (1 action) or first to the stack
and then the output (2 actions), the total number of
actions for a sequence of length n is maximally 2n.

数据结构：  
output： chunk space  
stack： scrach space  
buffer： 没有处理的word  

操作：  
SHIFT： buffer -> stack  
OUT: buffer -> output  
REDUCE: pop, label for all, -> output  

结束：  
stack为空，buffer为空。  

根据“output, stack, buffer, 操作历史”，决定“这一次采取某个操作的概率”。  
REDUCE时用双向RNN分析这组词的label。  

# 输入向量x是什么来的？  

每个单词的向量化表示  
难点：为NER生成独立的表示很难  

设计原理：  
1. 基于orthographic(拼写的)、morphological（形态学的）的分析，所以：  
使用与单词拼写有关的表达，即字符级表示。  
2. 虽然name本身会变化，但appear in regular context，所以：  
使用与单词order有关的表示
3. dropout

## 字符级的embedding

优势：  
形态学rich的语言  
处理out of vocabulary问题  
dependency parsing  

字符级embedding也是用的双向LSTM，然后两边的结果拼到一起。区别是每个LSTM都是在一个单词结束时才输出。  
再加上单词级的embedding，来自查表和UNK embedding  

为什么用双向LSTM？  
因为RNN/LSTM受最近的点影响特别大。双向分别用于识别单词的前缀和后缀。此处不适合用CNN，因为前缀和后缀的位置固定，不具有平移不变性。  

单词级的look-up table使用pretrained代替随机初始化，性能有明显提升。  
skip-n-gram + fine-tuned    

字符级表示 + 单词级表示 + pretrained = 效果一般  
字符级表示 + 单词级表示 + pretrained + dropout = 效果一般  


# 训练  

BP + online + SGD + gradient clipping