> **[info]** paper: http://aclweb.org/anthology/N/N16/N16-1030.pdf

# Abstract

State-of-the-art named entity recognition systems rely heavily on hand-crafted features and domain-specific knowledge in order to learn
effectively from the small, supervised training corpora that are available. In this paper, we introduce two new neural architectures—one
based on bidirectional LSTMs and conditional random fields, and the other that constructs and labels segments using a transition-based
approach inspired by shift-reduce parsers. Our models rely on two sources of infor-mation about words: character-based word representations learned from the supervised corpus and unsupervised word representations learned from unannotated corpora. Our models obtain state-of-the-art performance in
NER in four languages without resorting to any language-specific knowledge or resources such as gazetteers.

> **[success]**   
Name Entity Recognition， NER，命名实体识别问题
https://blog.csdn.net/suoyan1539/article/details/79550889
传统方法：基于人肉选feature和领域知识  
新方法：  
1. 双向LSTM + [CRF](https://windmissing.github.io/LiHang-TongJiXueXiFangFa/Chapter11/crf.html)  
2. 移位归约分析 + 基于转移的方法  
优点：性能好，且不需要语言相关的知识  

# Introduction

Named entity recognition (NER) is a challenging learning problem. One the one hand, in most languages and domains, there is only a very small amount of supervised training data available. On the other, there are few constraints on the kinds of words that can be names, so generalizing from this small sample of data is difficult. As a result, carefully constructed orthographic features and language-specific knowledge resources, such as gazetteers, are widely used for solving this task. Unfortunately, language-specific resources and features are costly to develop in new languages and new domains, making NER a challenge to adapt. Unsupervised learning from unannotated corpora offers an alternative strategy for obtaining better generalization from small amounts of supervision. However, even systems that have relied extensively on unsupervised features (Collobert et al., 2011; Turian et al., 2010; Lin and Wu, 2009; Ando and Zhang, 2005b, inter alia) have used these to augment, rather than replace, hand-engineered features (e.g., knowledge about capitalization patterns and character classes in a particular language) and specialized knowledge resources (e.g., gazetteers).  

> **[success]**   
NER的Challenge：  
1. 带标注的训练样本少，name没有规律，因此难以识别name   
2. gazetteer是预定义的name资源，无法用于新语言、新领域  
3. 监督学习+非监督学习的方法仍摆脱不了gazetteer  

In this paper, we present neural architectures for NER that use no language-specific resources or features beyond a small amount of supervised training data and unlabeled corpora. Our models are designed to capture two intuitions. First, since names often consist of multiple tokens, reasoning jointly over tagging decisions for each token is important. We compare two models here, (i) a bidirectional LSTM with a sequential conditional random layer above it (LSTM-CRF; §2), and (ii) a new model that constructs and labels chunks of input sentences using an algorithm inspired by transition-based parsing with states represented by stack LSTMs (S-LSTM; §3). Second, token-level evidence for “being a name” includes both orthographic evidence (what does the word being tagged as a name look like?) and distributional evidence (where does the word being tagged tend to occur in a corpus?). To capture orthographic sensitivity, we use character-based word representation model (Ling et al., 2015b) to capture distributional sensitivity, we combine these representations with distributional representations (Mikolov et al., 2013b). Our word representations combine both of these, and dropout training is used to encourage the model to learn to trust both sources of evidence (§4).

> **[success]**   
本文资源：  
（1） 少量的带标注data  
（2） 未标注的corpora  
（3） 无language-specific资源/知识  
先验：  
（1） name都是多token的  
（2） token-level evidence for "being a name"  
词表示：  
（1） 正交敏感性 --> 字符级词表示  
（2） 分布敏感性 --> 分布表示  
dropout: 信任所有source  

Experiments in English, Dutch, German, and Spanish show that we are able to obtain state-of-the-art NER performance with the LSTM-CRF model in Dutch, German, and Spanish, and very near the state-of-the-art in English without any hand-engineered features or gazetteers (§5). The transition-based algorithm likewise surpasses the best previously published results in several languages, although it performs less well than the LSTM-CRF model.

# LSTM - CRF模型

We provide a brief description of LSTMs and CRFs, and present a hybrid tagging architecture. This architecture is similar to the ones presented by Collobert et al. (2011) and Huang et al. (2015).  

## LSTM

Recurrent neural networks (RNNs) are a family of neural networks that operate on sequential data. They take as input a sequence of vectors (x 1 ,x 2 ,...,x n ) and return another sequence (h 1 ,h 2 ,...,h n ) that represents some information about the sequence at every step in the input. Although RNNs can, in theory, learn long dependencies, in practice they fail to do so and tend to be biased towards their most recent inputs in the sequence (Bengio et al., 1994). Long Short-term Memory Networks (LSTMs) have been designed to combat this issue by incorporating a memory-cell and have been shown to capture long-range dependencies. They do so using several gates that control the proportion of the input to give to the memory cell, and the proportion from the previous state to forget (Hochreiter and Schmidhuber, 1997). We use the following implementation:

> **[success]**   
输入向量序列x分别正向LSTM和反向LSTM得到向量h1和向量h2，  
h1和h2合并成一向量，称为h，是关于输入序列的some information  

$$
i_t = \sigma(W_{xi}x_t + W_{hi}h_{t-1} + W_{ci}c_{t-1} + b_i)  \\
c_t = (1-i_t)\odot c_{t-1} + i_t \odot \tanh(W_{xc}x_t + W_{hc}h_{t-1} + b_c)  \\
o_t = \sigma(W_{xo}x_t + W_{ho}h_{t-1} + W_{co}c_t + b_o)   \\
h_t = o_t \odot \tanh(c_t)
$$

> **[success]**   
LSTM的过程：  
1. 把$x_t$和$h_{t-1}$合并成一个大的输入向量$[x_t, h_{t-1}]$   
2. 根据输入向量$[x_t, h_{t-1}]$和缓存状态$C_{t-1}$计算gate  
3. gate决定输入向量$[x_t, h_{t-1}]$和缓存状态$C_{t-1}$各起多少作用，得到新的缓存状态$C_t$  
4. 根据输入向量$[x_t, h_{t-1}]$和新的缓存状态$C_t$计算另一个gate  
5. gate决定有多个$C_t$进入$h_t$

where σ is the element-wise sigmoid function, and $\odot$ is the element-wise product.
For a given sentence (x 1 ,x 2 ,...,x n ) containing n words, each represented as a d-dimensional vector, an LSTM computes a representation $h_t$ of the left context of the sentence at every word t. Naturally, generating a representation of the right context $h_t$ as well should add useful information. This can be achieved using a second LSTM that reads the same sequence in reverse. We will refer to the former as the forward LSTM and the latter as the backward LSTM. These are two distinct networks with different parameters. This forward and backward LSTM pair is referred to as a bidirectional LSTM (Graves and Schmidhuber, 2005).  

The representation of a word using this model is obtained by concatenating its left and right context representations, $h_t = [h_t;h_t]$. These representations effectively include a representation of a word in context, which is useful for numerous tagging applications.

## CRF Tagging Models

> **[success]**  
CRF 条件随机场
$$
P(Y|X)
$$

> 说明：  
公式代表在已知X的情况下预测Y的分布。  
X是输入变量，即观测序列   
Y是输出变量，即标记序列   

A very simple—but surprisingly effective—tagging model is to use the h t ’s as features to make independent tagging decisions for each output y t (Ling et al., 2015b). Despite this model’s success in simple problems like POS tagging, its independent classification decisions are limiting when there are strong dependencies across output labels. NER is one such task, since the “grammar” that characterizes interpretable sequences of tags imposes several hard constraints (e.g., I-PER cannot follow B-LOC; see §2.4 for details) that would be impossible to model with independence assumptions.

> **[success]**  
由于output之间有strong dependency，不能直接根据$h_t$预测$y_t$  
[?] Pos tagging

Therefore, instead of modeling tagging decisions independently, we model them jointly using a conditional random field (Lafferty et al., 2001). For an input sentence

> **[success]**  
解决方法：
使用CRF做tagging决定“jointly”

$$
X = (x_1, x_2, \cdots, x_n),
$$

we consider P to be the matrix of scores output by the bidirectional LSTM network. P is of size n × k, where k is the number of distinct tags, and $P_{i,j}$ corresponds to the score of the j th tag of the i th word in a sentence. For a sequence of predictions   
$$
y = (y 1 ,y 2 ,...,y n ),
$$

we define its score to be   
$$
s(X,y) = \sum_{i=0}^n A_{y_i, y_{i+1}} + \sum_{i=1}^n P_{i, y_i}
$$

> **[success]**  
P是$n \times k$的矩阵，  
$P_{ij}$代表第i个单词打第j个tag的分数   
n代表句子长度，k代表tag个数  
公式说明：  
x: 输入序列  
y: 预测序列  
第一项：tag前后的关联性分数  
第二项：独立分类的分数  
A：$(k+2) \times (k+2)$状态转移矩阵  
$A_{ij}$代表tag i向tag j转移的分数  
矩阵A中增加了start和end两个tag，因此size是k+2  

where A is a matrix of transition scores such that A i,j represents the score of a transition from the tag i to tag j. y 0 and y n are the start and end tags of a sentence, that we add to the set of possible tags. A is therefore a square matrix of size k+2.

A softmax over all possible tag sequences yields a probability for the sequence y:

$$
p(y|x) = \frac{e^{s(X, y)}}{\sum_{\tilde y \in Y_X} e^{s(X, \tilde y)}}
$$

> **[success]**  
$\sum$代表遍历所有的可能的y

During training, we maximize the log-probability of the correct tag sequence:  
$$
\begin{aligned}
\log(p(y|X)) &=& s(X, y) - \log\left(\sum_{\tilde y \in Y_X} e^{s(X, \tilde y)}\right)  \\
&=& s(X, y) - \text{logadd}_{\tilde y \in Y_X} s(X, \tilde y), && (1)
\end{aligned}
$$

> **[success]**   
把上面的公式对数化，以防下溢？  

where Y X represents all possible tag sequences (even those that do not verify the IOB format) for a sentence X. From the formulation above, it is evident that we encourage our network to produce a valid sequence of output labels. While decoding, we predict the output sequence that obtains the maximum score given by:  

$$
y^* = \arg\max s(X, \tilde y)
$$

> **[success]**   
在所有y中找到一个使s最大的y  

Since we are only modeling bigram interactions between outputs, both the summation in Eq. 1 and
the maximum a posteriori sequence y ∗ in Eq. 2 can be computed using dynamic programming.

> **[success]**   
bigram interaction是指$s_{i+1}$只依赖于$s_i$的结果，因此可以用DP   

## Parameterization and Training

The scores associated with each tagging decision for each token (i.e., the P i,y ’s) are defined to be the dot product between the embedding of a word-in-context computed with a bidirectional LSTM—exactly the same as the POS tagging model of Ling et al. (2015b) and these are combined with bigram compatibility scores (i.e., the A y,y 0 ’s). This architecture is shown in figure 1. Circles represent observed variables, diamonds are deterministic functions of their parents, and double circles are random variables.
![](/NANER/assets/2.png)  

> **[success]**   
![](/NANER/assets/1.png)  
虚线圆圈代表向量，实线圆圈代表unit。  
x为输入层  
l为正向LSTM的一个时间步    
r为反向LSTM的一个时间步  
f为一个普通的隐藏层    
c为crf层    
y为输出层  
用crf层代表softmax层生成y。   

The parameters of this model are thus the matrix of bigram compatibility scores A, and the parameters that give rise to the matrix P, namely the parameters of the bidirectional LSTM, the linear feature weights, and the word embeddings. As in part 2.2, let x i denote the sequence of word embeddings for every word in a sentence, and y i be their associated tags. We return to a discussion of how the embeddings x i are modeled in Section 4. The sequence of word embeddings is given as input to a bidirectional LSTM, which returns a representation of the left and right context for each word as explained in 2.1.

> **[success]**   
要训练的参数：  
A、双向LSTM的参数  

These representations are concatenated (c i ) and linearly projected onto a layer whose size is equal to the number of distinct tags. Instead of using the softmax output from this layer, we use a CRF as previously described to take into account neighboring tags, yielding the final predictions for every word y i . Additionally, we observed that adding a hidden layer between c i and the CRF layer marginally improved our results. All results reported with this model incorporate this extra-layer. The parameters are trained to maximize Eq. 1 of observed sequences of NER tags in an annotated corpus, given the observed words.

## Tagging Schemes

The task of named entity recognition is to assign a named entity label to every word in a sentence. A single named entity could span several tokens within a sentence. Sentences are usually represented in the IOB format (Inside, Outside, Beginning) where every token is labeled as B-label if the token is the beginning of a named entity, I-label if it is inside a named entity but not the first token within the named entity, or O otherwise. However, we decided to use the IOBES tagging scheme, a variant of IOB commonly used for named entity recognition, which encodes information about singleton entities (S) and explicitly marks the end of named entities (E). Using this scheme, tagging a word as I-label with high-confidence narrows down the choices for the subsequent word to I-label or E-label, however, the IOB scheme is only capable of determining that the subsequent word cannot be the interior of another label. Ratinov and Roth (2009) and Dai et al. (2015) showed that using a more expressive tagging scheme like IOBES improves model performance marginally. However, we did not observe a significant improvement over the IOB tagging scheme.

> **[success]**   
IOB format: 
Inside, Outside, Beginning  
IOBES tagging scheme：  
Single Entity, End of Entity

# Transition-Based Chunking Model

> **[success]**   
transition-based dependency parsing 基于转移的依赖分析

As an alternative to the LSTM-CRF discussed in the previous section, we explore a new architecture that chunks and labels a sequence of inputs using an algorithm similar to transition-based dependency parsing. This model directly constructs representations of the multi-token names (e.g., the name Mark Watney is composed into a single representation).

This model relies on a stack data structure to incrementally construct chunks of the input. To obtain representations of this stack used for predicting subsequent actions, we use the Stack-LSTM presented by Dyer et al. (2015), in which the LSTM is augmented with a “stack pointer.” While sequential LSTMs model sequences from left to right, stack LSTMs permit embedding of a stack of objects that are both added to (using a push operation) and removed from (using a pop operation). This allows the Stack-LSTM to work like a stack that maintains a “summary embedding” of its contents. We refer to this model as Stack-LSTM or S-LSTM model for simplicity.

> **[success]**   
stack - LSTM = LSTM + 栈顶指针    
普通LSTM： 从左往右依次加数据  
栈LSTM： 栈顶可以增加或移除数据，栈中的信息可以看作是内容的summary embedding  
[?] arc-standard parser  

Finally, we refer interested readers to the original paper (Dyer et al., 2015) for details about the Stack-LSTM model since in this paper we merely use the same architecture through a new transition-based algorithm presented in the following Section.

## Chunking Algorithm

We designed a transition inventory which is given in Figure 2 that is inspired by transition-based parsers, in particular the arc-standard parser of Nivre (2004). In this algorithm, we make use of two stacks (designated output and stack representing, respectively, completed chunks and scratch space) and a buffer that contains the words that have yet to be processed. The transition inventory contains the following transitions: The SHIFT transition moves a word from the buffer to the stack, the OUT transition moves a word from the buffer directly into the output stack while the REDUCE (y) transition pops all items from the top of the stack creating a “chunk,” labels this with label y, and pushes a representation of this chunk onto the output stack. The algorithm com-
pletes when the stack and buffer are both empty. The algorithm is depicted in Figure 2, which shows the sequence of operations required to process the sentence Mark Watney visited Mars.

The model is parameterized by defining a probability distribution over actions at each time step, given the current contents of the stack, buffer, and output, as well as the history of actions taken. Following Dyer et al. (2015), we use stack LSTMs to compute a fixed dimensional embedding of each of these, and take a concatenation of these to obtain the full algorithm state. This representation is used to define a distribution over the possible actions that can be taken at each time step. The model is trained to maximize the conditional probability of
sequences of reference actions (extracted from a la-
beled training corpus) given the input sentences. To
label a new input sequence at test time, the maxi-
mum probability action is chosen greedily until the
algorithm reaches a termination state. Although this
is not guaranteed to find a global optimum, it is ef-
fective in practice. Since each token is either moved
directly to the output (1 action) or first to the stack
and then the output (2 actions), the total number of
actions for a sequence of length n is maximally 2n.

数据结构：  
output： chunk space  
stack： scrach space  
buffer： 没有处理的word  

操作：  
SHIFT： buffer -> stack  
OUT: buffer -> output  
REDUCE: pop, label for all, -> output  

结束：  
stack为空，buffer为空。  

根据“output, stack, buffer, 操作历史”，决定“这一次采取某个操作的概率”。  
REDUCE时用双向RNN分析这组词的label。  

# 输入向量x是什么来的？  

每个单词的向量化表示  
难点：为NER生成独立的表示很难  

设计原理：  
1. 基于orthographic(拼写的)、morphological（形态学的）的分析，所以：  
使用与单词拼写有关的表达，即字符级表示。  
2. 虽然name本身会变化，但appear in regular context，所以：  
使用与单词order有关的表示
3. dropout

## 字符级的embedding

优势：  
形态学rich的语言  
处理out of vocabulary问题  
dependency parsing  

字符级embedding也是用的双向LSTM，然后两边的结果拼到一起。区别是每个LSTM都是在一个单词结束时才输出。  
再加上单词级的embedding，来自查表和UNK embedding  

为什么用双向LSTM？  
因为RNN/LSTM受最近的点影响特别大。双向分别用于识别单词的前缀和后缀。此处不适合用CNN，因为前缀和后缀的位置固定，不具有平移不变性。  

单词级的look-up table使用pretrained代替随机初始化，性能有明显提升。  
skip-n-gram + fine-tuned    

字符级表示 + 单词级表示 + pretrained = 效果一般  
字符级表示 + 单词级表示 + pretrained + dropout = 效果一般  


# 训练  

BP + online + SGD + gradient clipping