# Abstract

In this work we explore recent advances in Recurrent Neural Networks for **large scale Language Modeling**, a task central to language understanding. We extend current models to deal with two key challenges present in this task: corpora and vocabulary sizes, and complex, long term structure of language. We perform an exhaustive study on techniques such as character Convolutional Neural Networks or Long-Short Term Memory, on the One Billion Word Benchmark.   

> **[success]**  
task：language understanding  
challenges：（1）corpora and vocabulary sizes（2）complex, long term structure of language  
techniques：（1）character Convolutional Neural Networks （2）Long-Short Term Memory

Our best single model significantly improves state-of-the-art perplexity from 51.3 down to 30.0 (whilst reducing the number of parameters by a factor of 20), while an ensemble of models sets a new record by improving perplexity from 41.0 down to 23.7.   

> **[info]** perplexity：混乱  

We also release these models for the NLP and ML community to study
and improve upon.
