# Paper title

Enhancing Machine Translation with Dependency-Aware Self-Attention

# link

https://www.aclweb.org/anthology/2020.acl-main.147.pdf

# 摘要

Most neural machine translation models only rely on pairs of parallel sentences, assuming syntactic information is automatically learned by an attention mechanism. In this work, we investigate different approaches to incorporate syntactic knowledge in the Transformer model and also propose a novel, parameter-free, dependency-aware self-attention mechanism that improves its translation quality, especially for long sentences and in low-resource scenarios. We show the efficacy of each approach on WMT English↔German and English→Turkish, and WAT English→Japanese translation tasks. 

# 要解决什么问题

大多数据NMT模型没有考虑parallel sentences的句法信息。

# 作者的主要贡献

用不同的方法向NMT模型中加入句法信息。  
提出一种新颖的，无参数的，具有依赖项意识的自我注意机制来提高其翻译质量。  
新方法对长句子和low-resource场景特别有效。  

# 得到了什么结果

WMT English↔German  
WMT English→Turkish  
WAT English→Japanese  

# 关键字	

长句子
