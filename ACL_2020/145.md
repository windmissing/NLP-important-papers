# Paper title

Character-Level Translation with Self-attention

# link

https://www.aclweb.org/anthology/2020.acl-main.145.pdf

# 摘要

We explore the suitability of self-attention models for character-level neural machine translation. We test the standard transformer model, as well as a novel variant in which the encoder block combines information from nearby characters using convolutions. We perform extensive experiments on WMT and UN datasets, testing both bilingual and multilingual translation to English using up to three input languages (French, Spanish, and Chinese). Our transformer variant consistently outperforms the standard transformer at the character-level and converges faster while learning more robust character-level alignments.1 

# 要解决什么问题

将self-attention模型用于字符级NMT中。

# 作者的主要贡献

在encoder block中结合由CNN中附件字符中提取出的特征。

# 得到了什么结果

性能比其它字符级模型好。  
收敛速度比其它字符级模型快。  

# 关键字	
