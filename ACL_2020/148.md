# Paper title

Improving Massively Multilingual Neural Machine Translation and Zero-Shot Translation	

# link

https://www.aclweb.org/anthology/2020.acl-main.148.pdf	

# 摘要

Massively multilingual models for neural machine translation (NMT) are theoretically attractive, but often underperform bilingual models and deliver poor zero-shot translations. In this paper, we explore ways to improve them. We argue that multilingual NMT requires stronger modeling capacity to support language pairs with varying typological characteristics, and overcome this bottleneck via language-specific components and deepening NMT architectures. We identify the off-target translation issue (i.e. translating into a wrong target language) as the major source of the inferior zero-shot performance, and propose random online backtranslation to enforce the translation of unseen training language pairs. Experiments on OPUS-100 (a novel multilingual dataset with 100 languages) show that our approach substantially narrows the performance gap with bilingual models in both oneto-many and many-to-many settings, and improves zero-shot performance by ∼10 BLEU, approaching conventional pivot-based methods.	

backtranslation：反向翻译  


# motivation

多语言翻译是指用一个NMT模型来做多个语言之间的翻译。  

多语言NMT的优点：  
1. 便于模型部署  
2. 促进相关语言之间的知识迁移  
3. 提升low-resource翻译  
4. 使zero-shot翻译成为可能  

多语言NMT存在的问题：  
问题1：多语言NMT性能差于双语NMT。	  
问题2：多语言NMT处理zero-shot数据时（相对于pivot-based模型）会出现“off-target translation问题”，即翻译成一个错误的语言。例如某个NMT模型涉及ABCDE五种语言，那么这五种语言之间应该可以互相翻译。但是如果训练集中只有A->DE的样本，没有A->BC的样本。那么把A语言翻译成C语言会出错，会翻译成D或者E。  
[?] pivot-based methods  

# 已有的解决问题的方法

## 针对问题1：

1. 每个语言都有对应的encoder/decoder  

例如：  
一对多翻译，共享encoder  
多对多翻译，多个语言共享attention mechanism  

缺点：scalability受到限制。  

2. 把不同语言映射到同一个表示空间  

例如：  
with a target language symbol guiding the translation direction

缺点：  
忽略了不同语言的linguistic diversity  

3. 在2的基础上，加入“语言的linguistic diversity”的考虑

例如：  
reorganizing parameter sharing  
designing language-specific parameter generator  
decoupling multilingual word encoding  

**本文是以2为baseline探索3的方法**  

## 针对问题2：  

多语言NMT处理zero-shot数据时（相对于pivot-based模型）会出现“off-target translation问题”，即翻译成一个错误的语言。  

出现问题的原因：  
1. missing ingredient problem  
2. spurious correlation issue  

解决方法：  
1. 跨语言正则化  
2. generating artificial parallel data with backtranslation  

**本文探索3的方法来解决zero-shot问题**

# 作者的主要贡献

## 针对问题1：  

作者认为造成问题1的原因是模型容量的不足。因此具体做了以下改进：  
（1）增加transformer的深度  
（2）language-aware层归一化，即每种语言不共享transformer中layer normalization的参数    
（3）在encoder和decoder之间增加一层linear transformation layer，且不同的target languate不共享这个layer的参数

## 针对问题2：  

作者提供random online backtranslation （ROBT）算法  
首先要用一个已经训练好的多语言NMT模型，生成pseudo parallel样本的方法如下：  
（1）x -> t -> y  
（2）y -> t' -> x'  
（3）x' -> t -> y  
根据以上步骤得到了x'->y的训练样本。  
[?] 这里面t是一种中间语言，为什么会需要t呢？  
[?] Random和Online分别是怎么体现的？  
[?] prior study需要decode整个训练集，是什么意思？  


## 提供OPUS-100数据集：  
（1）55M条句子对  
（2）包含100种语言  

# 得到了什么结果

## 测试场景

baseline: [transformer](https://windmissing.github.io/NLP-important-papers/AIAYN/1.html) for 多语言NMT，pivot-based translation for zero-shot
数据：OPUS-100  
单词编码：[?]BPE  
evaluation：average [BLEU](https://windmissing.github.io/DeepLearningNotes/CaseStudy/Seq2Seq.html#%E8%AF%84%E4%BB%B7%E7%BF%BB%E8%AF%91%E7%BB%93%E6%9E%9C---bleu)

## 测试结果

（1） 未使用language-aware层归一化，单语言->多语言，性能恶化1.95。使用language-aware层归一化，性能恶化0.83。  
增加模型容量可以提升性能，减少多语言NMT与双语NMT之间的gap  
2. language-specific模型和尝试NMT能提升zero-shot的性能，但对解决off-target tranlation问题没有帮助  
3. ROBT算法减少off-target出现的概率，在zero-shot问题上性能比pivot-based methods提升10 BLEU。

# 关键字	

zero-shot、多语言翻译
