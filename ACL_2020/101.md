# Paper title

Towards Faithful Neural Table-to-Text Generation with Content-Matching Constraints

# link

https://www.aclweb.org/anthology/2020.acl-main.101.pdf

# 摘要

Text generation from a knowledge base aims to translate knowledge triples to naturallanguage descriptions. Most existing methods ignore the faithfulness between a generated text description and the original table, leading to generated information that goes beyond the content of the table. In this paper, for the first time, we propose a novel Transformerbased generation framework to achieve the goal. The core techniques in our method to enforce faithfulness include a new table-text optimal-transport matching loss and a tabletext embedding similarity loss based on the Transformer model. Furthermore, to evaluate faithfulness, we propose a new automatic metric specialized to the table-to-text generation problem. We also provide detailed analysis on each component of our model in our experiments. Automatic and human evaluations show that our framework can significantly outperform state-of-the-art by a large margin.

# 要解决什么问题

Table-to-Text中的大多数现有方法都忽略了生成的文本描述和原始表之间的真实性，从而导致生成的信息超出了表的内容。  

# 作者的主要贡献

基于transformed的生成框架：  
实现Table-to-Text的文本生成，使用table-text optimal-transport matching loss和tabletext embedding similarity loss来保证faithfulness。  

# 得到了什么结果

比SOTA好很多。  

# 关键字

faithfulness， table-to-text