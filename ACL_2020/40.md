# Paper title

Multiscale Collaborative Deep Models for Neural Machine Translation

# link

https://www.aclweb.org/anthology/2020.acl-main.40.pdf

# 摘要

Recent evidence reveals that Neural Machine Translation (NMT) models with deeper neural networks can be more effective but are difficult to train. In this paper, we present a MultiScale Collaborative (MSC) framework to ease the training of NMT models that are substantially deeper than those used previously. We explicitly boost the gradient backpropagation from top to bottom levels by introducing a block-scale collaboration mechanism into deep NMT models. Then, instead of forcing the whole encoder stack directly learns a desired representation, we let each encoder block learns a fine-grained representation and enhance it by encoding spatial dependencies using a context-scale collaboration. We provide empirical evidence showing that the MSC nets are easy to optimize and can obtain improvements of translation quality from considerably increased depth. On IWSLT translation tasks with three translation directions, our extremely deep models (with 72-layer encoders) surpass strong baselines by +2.2∼+3.1 BLEU points. In addition, our deep MSC achieves a BLEU score of 30.56 on WMT14 English→German task that significantly outperforms state-of-the-art deep NMT models.

# 要解决什么问题

用深度神经网络训练NMT难以训练。

# 作者的主要贡献

引入block-scale collabration机制，具体做法如下：  
把整个encoder stack分布多个encoder block，每个encoder block学习一个fine-grained表示。  
使用context-scale collabration对spatial dependency进行编码来增强上述表示。

# 得到了什么结果

网络变深时容易优化。  
在深层网络结构上，性能优于Benchmark。  
 WMT14 English→German     BLEU     30.56

# 关键字

Encoder, deep
