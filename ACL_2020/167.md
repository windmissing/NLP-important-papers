# Paper title

GPT-too: A Language-Model-First Approach for AMR-to-Text Generation

# link

https://www.aclweb.org/anthology/2020.acl-main.167.pdf
 
# 摘要

Abstract Meaning Representations (AMRs) are broad-coverage sentence-level semantic graphs. Existing approaches to generating text from AMR have focused on training sequenceto-sequence or graph-to-sequence models on AMR annotated data only. In this paper, we propose an alternative approach that combines a strong pre-trained language model with cycle consistency-based re-scoring. Despite the simplicity of the approach, our experimental results show these models outperform all previous techniques on the English LDC2017T10 dataset, including the recent use of transformer architectures. In addition to the standard evaluation metrics, we provide human evaluation experiments that further substantiate the strength of our approach. 

# 要解决什么问题

现有的从AMR生成文本的方法仅集中于在AMR注释数据上训练seq2seq或graph2seq模型。

# 作者的主要贡献

将预训练语言模型与基于周期一致性的re-scoring相结合。  
[?] re-scoring

# 得到了什么结果

English LDC2017T10 dataset
     自动评估    超过所有现有的模型
     人工评估    新方法更有优势
     
# 关键字