# Neural Machine Translation

A  neural  machine  translation  system  is  a  neural network that **directly models the conditional probability p(y|x) of  translating  a  source  sentence,x1, . . . , xn,  to a target  sentence,y1, . . . , ym**.3 A basic form of NMT consists of two components:(a) an encoder which computes a representations for each source sentence and (b) a decoder which generates one target word at a time and hence decomposes the conditional probability as:  
$$
\begin{aligned}
\log p(y|x) =\sum^m_{j=1}\log p(y_j|y_{<j},s)  && (1)
\end{aligned}
$$

> **[success]**  
NMT是指计算给定输入序列对应输出序列的概率分布。  
NMT模型分为encoder和decoder两部分。  
encoder用于把输入序列的信息打包。  
decoder用于生成另一个序列。  
以上公式是decoder的公式。  

A   natural    choice    to   model    such    a   decomposition    in    the    decoder    is    to    use    recurrent neural network(RNN)architecture,   which   most   of   the   recent   NMT   work such as(Kalchbrenner and Blunsom, 2013;Sutskever et al., 2014;Cho et al., 2014;Bahdanau et al., 2015;Luong et al., 2015;Jean et al., 2015)  have  in  common.   They,  however,  differ in terms of which RNN architectures are  used  for  the  decoder  and  how  the  encoder computes the source sentence representation $s$.  

> **[success]**  
encoder：不同的模型对s的编码方式不同。  
decoder：NMT常用docoder模型是RNN，但在RNN的结构上有所不同   

Kalchbrenner and Blunsom (2013)used an RNN   with   the   standard   hidden   unit   for   the decoder  and  a  convolutional  neural  network  for encoding the source sentence representation.   

> **[success]** encoder: CNN, decoder: RNN(standard unit)  

On the  other  hand,  both  Sutskever et al. (2014)  and Luong et al. (2015) stacked  multiple layers  of anRNN with a Long Short-Term  Memory  (LSTM)hidden unit for both the encoder and the decoder.  

> **[success]** encoder & decoder：多层LSTM  

Cho et al. (2014),Bahdanau et al. (2015),andJean et al. (2015) all adopted a different version ofthe RNN with an LSTM-inspired hidden unit, thegated recurrent unit (GRU), for both components.4  

> **[success]** encoder & decoder：多层GRU    

In more detail, one can parameterize the probability of decoding each word yj as:  
$$
\begin{aligned}
p(y_j|y_{<j},s) = softmax (g(h_j))  &&  (2)
\end{aligned}
$$

with g being the **transformation function** that outputs a vocabulary-sized  vector.5 Here,hj is the RNN hidden unit, abstractly computed as:  
$$
\begin{aligned}
h_j=f(h_{j−1},s) && (3)
\end{aligned}
$$

where f computes   the   **current   hidden   state** given   the   previous   hidden   state   and   can   be either  a  vanilla  RNN  unit,  a  GRU, or  an  LSTMunit.In    (Kalchbrenner and Blunsom, 2013;Sutskever et al., 2014;Cho et al., 2014;Luong et al., 2015),thesourcerepresenta-tionsis   only   used   once   to   initialize   thedecoder  hidden  state.On  the  other  hand,   in(Bahdanau et al., 2015;Jean et al., 2015)andthis  work,s,  in  fact,  implies  a  set  of  sourcehidden states which are consulted throughout theentire course of the translation  process.  Such anapproach is referred to as an attention mechanism,which we will discuss next.In this work, following (Sutskever et al., 2014;Luong et al., 2015),  we  use  the  stacking  LSTMarchitecture  for  our  NMT systems,  as  illustrated4They all used a single RNN layer except for the latter twoworks which utilized a bidirectional RNN for the encoder.5One can providegwith other inputs such as the currentlypredicted wordyjas in (Bahdanau et al., 2015).
in  Figure  1.   We  use  the  LSTM  unit  defined  in(Zaremba et al., 2015).   Our  training  objective  isformulated as follows:Jt=∑(x,y)∈D−logp(y|x)(4)withDbeing our parallel training corpus.3    Attention-based ModelsOur various  attention-based  models  are classifedinto two broad categories,globalandlocal. Theseclasses differ in terms of whether the “attention”is placed on all source positions or on only a fewsource  positions.   We  illustrate  these  two  modeltypes in Figure 2 and 3 respectively.Common to these two types of models is the factthat at each time steptin the decoding phase, bothapproaches first take as input the hidden statehtat the top layer of a stacking LSTM. The goal isthen to derive a context vectorctthat captures rel-evant source-side  information  to help predict  thecurrent target wordyt.  While these models differin how the context vectorctis derived, they sharethe same subsequent steps.Specifically, given the target hidden statehtandthe  source-side  context  vectorct,  we  employ  asimple concatenation  layer to combine the infor-mation from both vectors to produce an attentionalhidden state as follows: ̃ht= tanh(Wc[ct;ht])(5)The attentional vector ̃htis then fed through thesoftmax  layer  to  produce  the  predictive  distribu-tion formulated as:p(yt|y<t, x) = softmax(Ws ̃ht)(6)We now detail how each model type computesthe source-side context vectorct.3.1Global AttentionThe idea of a global attentional  model is to con-sider all the hidden states of the encoder when de-riving  the  context  vectorct.   In this  model  type,a variable-length alignment vectorat, whose sizeequals the number of time steps on the source side,is derived by comparing the current target hiddenstatehtwith each source hidden state ̄hs:at(s) = align(ht, ̄hs)(7)=exp(score(ht, ̄hs))∑s′exp(score(ht, ̄hs′))yt ̃htctatht ̄hsGlobal align weightsAttention LayerContext vectorFigure 2:Global attentional model– at each timestept,  the  model  infers  avariable-lengthalign-ment weight vectoratbased on the current targetstatehtand all source states ̄hs. A global contextvectorctis then computed as the weighted aver-age, according toat, over all the source states.Here,scoreis referred as acontent-basedfunctionfor which we consider three different alternatives:score(ht, ̄hs)=h⊤t ̄hsdoth⊤tWa ̄hsgeneralv⊤atanh(Wa[ht; ̄hs])concatBesides, in our early attempts to build attention-based  models,  we  use  alocation-basedfunctionin which the alignment scores are computed fromsolely the target hidden statehtas follows:at= softmax(Waht)location(8)Given the alignment vector as weights, the contextvectorctis computed as the weighted average overall the source hidden states.6Comparison to (Bahdanau et al., 2015)– Whileour  global  attention  approach  is  similar  in  spiritto the model proposed by Bahdanau et al. (2015),there are several key differences which reflect howwe  have  both  simplified  and  generalized  fromthe  original  model.First,  we  simply  use  hid-den  states  at  the  top  LSTM  layers  in  both  theencoder  and  decoder  as  illustrated  in  Figure  2.Bahdanau et al. (2015),  on  the  other  hand,   usethe  concatenation  of  the  forward  and  backwardsource hidden states in the bi-directional encoder6Eq. (8) implies that all alignment vectorsatare of thesame length. For short sentences, we only use the top part ofatand for long sentences, we ignore words near the end.
yt ̃htctathtpt ̄hsAttention LayerContext vectorLocal weightsAligned positionFigure 3:Local attention model– the model firstpredicts a single aligned positionptfor the currenttarget word. A window centered around the sourcepositionptis then used to compute a context vec-torct,  a  weighted  average  of  the  source  hiddenstates in the window.  The weightsatare inferredfrom the current target statehtand those sourcestates ̄hsin the window.and target hidden states in their non-stacking uni-directional decoder. Second, our computation pathis  simpler;  we  go  fromht→at→ct→ ̃htthen  make  a  prediction  as  detailed  in  Eq.  (5),Eq.  (6),  and  Figure  2.On  the  other  hand,  atany timet, Bahdanau et al. (2015) build from theprevious  hidden  stateht−1→at→ct→ht,  which,  in  turn,  goes  through  a  deep-outputand  a  maxout  layer  before  making  predictions.7Lastly, Bahdanau et al. (2015) only experimentedwith one alignment function,  theconcatproduct;whereas we show later that the other alternativesare better.3.2Local AttentionThe global attention has a drawback that it has toattend to all words on the source side for each tar-get word, which is expensive and can potentiallyrender it impractical to translate longer sequences,e.g.,  paragraphs  or  documents.   To  address  thisdeficiency,  we propose  alocalattentional  mech-anism that chooses to focus only on a small subsetof the source positions per target word.This model takes inspiration  from the tradeoffbetween thesoftandhardattentional models pro-posed by Xu et al. (2015) to tackle the image cap-tion generation task.  In their work, soft attention7We will refer to this difference again in Section 3.3.refers  to  the  global  attention  approach  in  whichweights are placed “softly” over all patches in thesource  image.   The  hard  attention,  on  the  otherhand, selects one patch of the image to attend to ata time. While less expensive at inference time, thehard attention model is non-differentiable and re-quires more complicated techniques such as vari-ance reduction or reinforcement learning to train.Our  local  attention  mechanism  selectively  fo-cuses on a small window of context and is differ-entiable. This approach has an advantage of avoid-ing the expensive computation incurred in the softattention  and  at  the  same  time,  is  easier  to  trainthan the hard attention approach.  In concrete de-tails, the model first generates an aligned positionptfor each target word at timet. The context vec-torctis then derived as a weighted average overthe set of source hidden states within the window[pt−D, pt+D];Dis empirically selected.8Unlikethe global approach, the local alignment vectoratis now fixed-dimensional, i.e.,∈R2D+1. We con-sider two variants of the model as below.Monotonicalignment (local-m) – we simply setpt=tassuming that source and target sequencesare roughly monotonically aligned. The alignmentvectoratis defined according to Eq. (7).9Predictivealignment (local-p) – instead of as-suming monotonic alignments, our model predictsan aligned position as follows:pt=S·sigmoid(v⊤ptanh(Wpht)),(9)Wpandvpare the model parameters which willbe learned to predict positions.Sis the source sen-tence length.  As a result ofsigmoid,pt∈[0, S].To  favor  alignment  points  nearpt,  we  place  aGaussian distribution centered aroundpt. Specif-ically, our alignment weights are now defined as:at(s) = align(ht, ̄hs) exp(−(s−pt)22σ2)(10)We use the samealignfunction as in Eq. (7) andthe standard deviation is empirically set asσ=D2.Note thatptis arealnummber;  whereassis anintegerwithin the window centered atpt.108If the window crosses the sentence boundaries, we sim-ply ignore the outside part and consider words in the window.9local-mis the same as the global model except that thevectoratis fixed-length and shorter.10local-pis similar to the local-m model except that we dy-namically computeptand use a truncated Gaussian distribu-tion to modify the original alignment weightsalign(ht, ̄hs)as shown in Eq. (10).  By utilizingptto deriveat, we cancompute backprop gradients forWpandvp.  This model isdifferentiable almost everywhere.
 ̃htAttention LayerBCD<eos>XYZXYZ<eos>AFigure 4:Input-feeding approach– Attentionalvectors ̃htare fed as inputs to the next time steps toinform the model about past alignment decisions.Comparison to (Gregor et al., 2015)– have pro-posed aselective attentionmechanism, very simi-lar to our local attention, for the image generationtask. Their approach allows the model to select animage patch  of varying  location  and zoom.   We,instead,  use the same “zoom” for all target posi-tions, which greatly simplifies the formulation andstill achieves good performance.3.3Input-feeding ApproachIn  our  proposed  global  and  local  approaches,the attentional decisions are made independently,which  is suboptimal.   Whereas,  in  standard  MT,acoverageset  is  often  maintained  during  thetranslation process to keep track of which sourcewords  have  been  translated.   Likewise,  in  atten-tional NMTs, alignment decisions should be madejointly  taking  into  account  past  alignment  infor-mation.    To  address  that,  we  propose  aninput-feedingapproach  in which attentional  vectors ̃htare concatenated with inputs at the next time stepsas  illustrated  in  Figure  4.11The  effects  of  hav-ing  such  connections  are  two-fold:  (a)  we  hopeto make the model fully aware of previous align-ment choices  and (b) we create a very deep net-work spanning both horizontally and vertically.Comparisontootherwork–Bahdanau et al. (2015)usecontextvectors,similar  to  ourct,  in  building  subsequent  hiddenstates,   which  can  also  achieve  the  “coverage”effect.  However, there has not been any analysisof  whether  such  connections  are  useful  as  done11Ifnis the number of LSTM cells, the input size of thefirst LSTM layer is2n; those of subsequent layers aren.in this work.  Also, our approach is more general;as  illustrated  in  Figure  4,  it  can  be  applied  togeneral stacking recurrent architectures, includingnon-attentional models.Xu et al. (2015)  propose  adoubly  attentionalapproach  with  an  additional  constraint  added  tothe training objective to make sure the model paysequal attention to all parts of the image during thecaption generation process.  Such a constraint canalso  be  useful  to  capture  the  coverage  set  effectin NMT that we mentioned earlier.  However, wechose  to  use  the  input-feeding  approach  since  itprovides flexibility for the model to decide on anyattentional constraints it deems suitable.4    ExperimentsWe   evaluate   the   effectiveness   of   our   modelson   the   WMT   translation   tasks   between   En-glish   and   German   in   both   directions.new-stest2013  (3000  sentences)  is used as a develop-ment set to select our hyperparameters.   Transla-tion  performances  are  reported  in  case-sensitiveBLEU   (Papineni et al., 2002)   on   newstest2014(2737  sentences)  and  newstest2015  (2169  sen-tences). Following (Luong et al., 2015), we reporttranslation  quality using two types of BLEU: (a)tokenized12BLEU to be comparable with existingNMT work and (b)NIST13BLEU to be compara-ble with WMT results.4.1    Training DetailsAll our models are trained on the WMT’14 train-ing data consisting of 4.5M sentences pairs (116MEnglish  words,  110M  German  words).    Similarto (Jean et al., 2015), we limit our vocabularies tobe the top 50K most frequent words for both lan-guages.  Words not in these shortlisted vocabular-ies are converted into a universal token<unk>.When  training  our  NMT  systems,  following(Bahdanau et al., 2015;  Jean et al., 2015),  we  fil-ter   out   sentence   pairs   whose   lengths   exceed50  words  and  shuffle  mini-batches  as  we  pro-ceed.    Our  stacking  LSTM  models  have  4  lay-ers,  each with 1000  cells,  and 1000-dimensionalembeddings.    We  follow  (Sutskever et al., 2014;Luong et al., 2015) in training NMT with similarsettings:  (a) our parameters are uniformly initial-ized in[−0.1,0.1], (b) we train for 10 epochs us-12All  texts  are  tokenized  withtokenizer.perlandBLEU scores are computed withmulti-bleu.perl.13With themteval-v13ascript as per WMT guideline.
SystemPplBLEUWinning WMT’14 system –phrase-based + large LM(Buck et al., 2014)20.7Existing NMT systemsRNNsearch (Jean et al., 2015)16.5RNNsearch + unk replace (Jean et al., 2015)19.0RNNsearch + unk replace + large vocab +ensemble8 models (Jean et al., 2015)21.6Our NMT systemsBase10.611.3Base + reverse9.912.6 (+1.3)Base + reverse + dropout8.114.0 (+1.4)Base + reverse + dropout + global attention (location)7.316.8 (+2.8)Base + reverse + dropout + global attention (location) + feed input6.418.1 (+1.3)Base + reverse + dropout + local-p attention (general) + feed input5.919.0 (+0.9)Base + reverse + dropout + local-p attention (general) + feed input + unk replace20.9 (+1.9)Ensemble8 models + unk replace23.0 (+2.1)Table 1:WMT’14 English-German results– shown are the perplexities (ppl) and thetokenizedBLEUscores of various systems on newstest2014.  We highlight thebestsystem in bold and giveprogressiveimprovements in italic between consecutive systems.local-preferes to the local attention with predictivealignments. We indicate for each attention model the alignment score function used in pararentheses.ing plain SGD, (c) a simple learning  rate sched-ule is employed – we start with a learning rate of1;  after 5 epochs,  we begin to halve the learningrate every epoch,  (d) our mini-batch  size is 128,and (e) the normalized gradient is rescaled when-ever  its  norm  exceeds  5.   Additionally,  we  alsouse dropout with probability0.2for our LSTMs assuggested by (Zaremba et al., 2015).  For dropoutmodels,  we train for 12 epochs and start halvingthe learning  rate after 8 epochs.   For local atten-tion models,  we empirically  set the window sizeD= 10.Our code is implemented  in MATLAB. Whenrunning  on  a  single  GPU  device  Tesla  K40,  weachieve  a  speed  of  1Ktargetwords  per  second.It takes 7–10 days to completely train a model.4.2English-German ResultsWe  compare  our  NMT  systems  in  the  English-German  task  with  various  other  systems.   TheseincludethewinningsysteminWMT’14(Buck et al., 2014),aphrase-basedsystemwhose  language  models  were  trained  on  a  hugemonolingual  text,   the  Common  Crawl  corpus.For  end-to-end  NMT  systems,   to  the  best  ofour   knowledge,   (Jean et al., 2015)   is   the   onlywork  experimenting  with  this  language  pair  andcurrently  the  SOTA  system.We  only  presentresults for some of our attention models and willlater analyze the rest in Section 5.As   shown   in   Table   1,    we   achieve   pro-gressive  improvements  when  (a)  reversing  thesource  sentence,   +1.3BLEU,  as  proposed   in(Sutskever et al., 2014)   and   (b)   using   dropout,+1.4BLEU. On top of that,  (c) the global atten-tion  approach  gives  a  significant  boost  of  +2.8BLEU, making our model slightly better than thebase attentional system of Bahdanau et al. (2015)(rowRNNSearch).When  (d)  using  theinput-feedingapproach,  we seize  another  notable  gainof +1.3BLEU and outperform their system.  Thelocal  attention  model  with  predictive  alignments(rowlocal-p)  proves  to  be  even  better,  givingus  a  further  improvement  of  +0.9BLEU on  topof  the  global  attention  model.It  is  interest-ing  to  observe  the  trend  previously  reported  in(Luong et al., 2015) that perplexity strongly corre-lates with translation quality.  In total, we achievea  significant  gain  of  5.0  BLEU  points  over  thenon-attentional  baseline,  which  already  includesknown  techniques  such  as  source  reversing  anddropout.The unknown replacement technique proposedin (Luong et al., 2015; Jean et al., 2015) yields an-other nice gain of +1.9BLEU, demonstrating thatour attentional models do learn useful alignmentsfor  unknown  works.    Finally,  by  ensembling  8different  models  of  various  settings,  e.g.,  usingdifferent  attention  approaches,  with  and  withoutdropout etc., we were able to achieve anew SOTAresult of23.0BLEU, outperforming  the existing
best system (Jean et al., 2015) by +1.4BLEU.SystemBLEUTop –NMT + 5-gram rerank(Montreal)24.9Our ensemble 8 models + unk replace25.9Table  2:WMT’15  English-German  results–NISTBLEU   scores   of   the   winning   entry   inWMT’15 and our best one on newstest2015.Latest results in WMT’15– despite the fact thatour models were trained on WMT’14 with slightlyless data, we test them on newstest2015 to demon-strate that they can generalize well to different testsets.   As  shown  in  Table  2,  our  best  system  es-tablishes anew SOTAperformance of25.9BLEU,outperforming the existing best system backed byNMT and a 5-gram LM reranker by +1.0BLEU.4.3German-English ResultsWe carry out a similar set of experiments for theWMT’15  translation  task  from  German  to  En-glish.   While  our  systems  have  not  yet  matchedthe performance  of the SOTA system,  we never-theless  show  the effectiveness  of our approacheswith large and progressive gains in terms of BLEUas  illustrated  in  Table  3.   Theattentionalmech-anism  gives  us  +2.2BLEU  gain  and  on  top  ofthat, we obtain another boost of up to +1.0BLEUfrom  theinput-feedingapproach.   Using a betteralignment function, the content-baseddotproductone, together withdropoutyields another gain of+2.7BLEU. Lastly, when applying the unknownword  replacement  technique,  we  seize  an  addi-tional  +2.1BLEU, demonstrating  the  usefulnessof attention in aligning rare words.5    AnalysisWe conduct extensive analysis to better understandour models in terms of learning, the ability to han-dle long sentences, choices of attentional architec-tures, and alignment quality.  All results reportedhere are on English-German newstest2014.5.1Learning curvesWe compare models built on top of one another aslisted in Table 1.  It is pleasant to observe in Fig-ure  5  a  clear  separation  between  non-attentionaland  attentional  models.The  input-feeding  ap-proach and the local attention model also demon-strate their abilities in driving the test costs lower.The non-attentional model with dropout (the blueSystemPpl.BLEUWMT’15 systemsSOTA –phrase-based(Edinburgh)29.2NMT + 5-gram rerank (MILA)27.6Our NMT systemsBase (reverse)14.316.9+ global (location)12.719.1 (+2.2)+ global (location) + feed10.920.1 (+1.0)+ global (dot) + drop + feed9.722.8 (+2.7)+ global (dot) + drop + feed + unk24.9 (+2.1)Table  3:WMT’15  German-English  results–performances  of  various  systems  (similar  to  Ta-ble 1).  Thebasesystem already includes sourcereversing   on   which   we   addglobalattention,dropout, inputfeeding, andunkreplacement.0.20.40.60.811.21.41.61.8x 10523456Mini−batchesTest costbasicbasic+reversebasic+reverse+dropoutbasic+reverse+dropout+globalAttnbasic+reverse+dropout+globalAttn+feedInputbasic+reverse+dropout+pLocalAttn+feedInputFigure 5:Learning curves– test cost (lnperplex-ity) on newstest2014  for English-German  NMTsas training progresses.+  curve)  learns  slower  than  other  non-dropoutmodels, but as time goes by, it becomes more ro-bust in terms of minimizing test errors.5.2    Effects of Translating Long SentencesWe  follow  (Bahdanau et al., 2015)  to  group  sen-tences  of  similar  lengths  together  and  computea  BLEU  score  per  group.    Figure  6  shows  thatour attentional models are more effective than thenon-attentional  one  in  handling  long  sentences:the quality does not degrade as sentences becomelonger.  Our best model (the blue + curve) outper-forms all other systems in all length buckets.5.3    Choices of Attentional ArchitecturesWe  examine  different  attention  models  (global,local-m,  local-p)  and  different  alignment  func-tions (location, dot, general, concat) as describedin Section  3.   Due to  limited  resources,  we can-not run all the possible  combinations.   However,results in Table 4 do give us some idea about dif-ferent choices.  Thelocation-basedfunction does
1020304050607010152025Sent LengthsBLEU     ours, no attn (BLEU 13.9)ours, local−p attn (BLEU 20.9)ours, best system (BLEU 23.0)WMT’14 best (BLEU 20.7)Jeans et al., 2015 (BLEU 21.6)Figure 6:Length Analysis– translation qualitiesof different systems as sentences become longer.SystemPplBLEUBeforeAfter unkglobal (location)6.418.119.3 (+1.2)global (dot)6.118.620.5 (+1.9)global (general)6.117.319.1 (+1.8)local-m (dot)>7.0xxlocal-m (general)6.218.620.4 (+1.8)local-p (dot)6.618.019.6 (+1.9)local-p (general)5.91920.9 (+1.9)Table  4:Attentional  Architectures–  perfor-mances of different attentional models. We trainedtwo local-m (dot) models; both have ppl>7.0.not learn  good alignments:  theglobal  (location)model  can  only  obtain  a  small  gain  when  per-forming unknown word replacement compared tousing  other  alignment  functions.14Forcontent-basedfunctions, our implementationconcatdoesnot  yield  good  performances  and  more  analysisshould  be done to understand  the reason.15It isinteresting to observe thatdotworks well for theglobal attention andgeneralis better for the localattention.  Among the different models,  the localattention model with predictive alignments (local-p) is best, both in terms of perplexities and BLEU.5.4Alignment QualityA by-product of attentional models are word align-ments.   While  (Bahdanau et al., 2015)  visualized14There  is  a  subtle  difference  in  how  we  retrieve  align-ments for the different alignment functions. At time steptinwhich we receiveyt−1as input and then computeht,at,ct,and ̃htbefore predictingyt, the alignment vectoratis usedas  alignment  weights  for  (a)  the  predicted  wordytin  thelocation-basedalignment  functions  and  (b)  the  input  wordyt−1in thecontent-basedfunctions.15Withconcat, the perplexities achieved by different mod-els are 6.7  (global),  7.1 (local-m),  and  7.1  (local-p).   Suchhigh perplexities could be due to the fact that we simplify thematrixWato set the part that corresponds to ̄hsto identity.MethodAERglobal (location)0.39local-m (general)0.34local-p (general)0.36ensemble0.34Berkeley Aligner0.32Table 6:AER scores– results of various modelson the RWTH English-German alignment data.alignments  for  some  sample  sentences  and  ob-served  gains  in  translation  quality  as  an  indica-tion of a working attention model, no work has as-sessed the alignments learned as a whole.  In con-trast, we set out to evaluate the alignment qualityusing the alignment error rate (AER) metric.Given  the  gold  alignment  data  provided  byRWTH  for  508  English-German  Europarl  sen-tences, we “force” decode our attentional modelsto produce translations that match the references.We extract only one-to-one alignments by select-ing  the  source  word  with  the  highest  alignmentweight per target word. Nevertheless, as shown inTable 6, we were able to achieve AER scores com-parable  to  the  one-to-many  alignments  obtainedby the Berkeley aligner (Liang et al., 2006).16We also found that the alignments produced bylocal  attention  models  achieve  lower  AERs thanthose of the global one. The AER obtained by theensemble, while good, is not better than the local-m  AER,  suggesting  the  well-known  observationthat AER and translation scores are not well cor-related (Fraser and Marcu, 2007).  We show somealignment visualizations in Appendix A.5.5    Sample TranslationsWe  show  in  Table  5  sample  translations  in  bothdirections.It  it  appealing  to  observe  the  ef-fect of attentional models in correctly translatingnames such as “Miranda Kerr” and “Roger Dow”.Non-attentional  models,  while  producing  sensi-ble  names  from  a  language  model  perspective,lack  the  direct  connections  from  the  source  sideto  make  correct  translations.   We  also  observedan interesting case in the second example, whichrequires  translating  thedoubly-negatedphrase,“not  incompatible”.   The  attentional  model  cor-rectly produces “nicht. . .unvereinbar”;  whereasthe non-attentional model generates “nicht verein-16We concatenate the 508 sentence pairs with 1M sentencepairs from WMT and run the Berkeley aligner.
English-German translationssrcOrlando Bloom and Miranda Kerr still love each otherrefOrlando Bloom undMiranda Kerrlieben sich noch immerbestOrlando Bloom undMiranda Kerrlieben einander noch immer .baseOrlando Bloom undLucas Mirandalieben einander noch immer .src′′We′re pleased the FAA recognizes that an enjoyable passenger experience is not incompatiblewith safety and security ,′′said Roger Dow , CEO of the U.S. Travel Association .ref“ Wir freuen uns , dass die FAA erkennt , dass ein angenehmes Passagiererlebnis nicht im Wider-spruch zur Sicherheit steht ” , sagteRoger Dow, CEO der U.S. Travel Association .best′′Wir freuen uns ,  dass die FAA anerkennt  ,  dass ein angenehmes  ist nicht mit Sicherheit  undSicherheitunvereinbarist′′, sagteRoger Dow, CEO der US - die .base′′Wir freuen uns  ̈uber die<unk>, dass ein<unk> <unk>mit Sicherheit nichtvereinbarist mitSicherheit und Sicherheit′′, sagteRogerCameron, CEO der US -<unk>.German-English translationssrcIn einem Interview sagte Bloom jedoch , dass er und Kerr sich noch immer lieben .refHowever , in an interview , Bloom has said that he andKerrstill love each other .bestIn an interview , however , Bloom said that he andKerrstill love .baseHowever , in an interview , Bloom said that he andTinawere still<unk>.srcWegen  der  von  Berlin  und  der  Europ ̈aischen  Zentralbank  verh ̈angten  strengen  Sparpolitik  inVerbindung  mit der Zwangsjacke  ,  in die die jeweilige nationale  Wirtschaft  durch  das Festhal-ten an der gemeinsamen W ̈ahrung gen  ̈otigt wird , sind viele Menschen der Ansicht , das ProjektEuropa sei zu weit gegangenrefTheausterity imposed by Berlin and the European Central Bank , coupled with the straitjacketimposed on national economies through adherence to the common currency , has led many peopleto think Project Europe has gone too far .bestBecause  of the strictausterity  measures  imposed  by Berlin and  the European  Central  Bank inconnection with the straitjacketin which the respective national economy is forced to adheretothe common currency , many people believe that the European project has gone too far .baseBecause of the pressureimposed by the European Central Bank and the Federal CentralBankwith the strict austerityimposed on the national economy in the face of the single currency ,many people believe that the European project has gone too far .Table 5:Sample translations– for each example, we show the source (src), the human translation (ref),the translation  from our best model (best),  and the translation  of a non-attentional  model (base).  Weitalicize somecorrecttranslation segments and highlight a fewwrongones in bold.bar”, meaning “not compatible”.17The attentionalmodel also demonstrates its superiority in translat-ing long sentences as in the last example.6    ConclusionIn this paper, we propose two simple and effectiveattentional mechanisms for neural machine trans-lation:   theglobalapproach  which  always  looksat all source positions and thelocalone that onlyattends  to a subset of source  positions  at a time.We  test  the  effectiveness  of  our  models  in  theWMT translation tasks between English and Ger-man in both directions.  Our local attention yieldslarge gains of up to5.0BLEU over non-attentional17The reference uses a more fancy translation of “incom-patible”, which is “im Widerspruch zu etwas stehen”.  Bothmodels, however, failed to translate “passenger experience”.models  which  already  incorporate  known  tech-niques such as dropout.   For the English  to Ger-man translation direction, our ensemble model hasestablished  new  state-of-the-art  results  for  bothWMT’14  and  WMT’15,  outperforming  existingbest systems, backed by NMT models andn-gramLM rerankers, by more than 1.0 BLEU.We have compared various alignment functionsand  shed  light  on  which  functions  are  best  forwhich attentional models. Our analysis shows thatattention-based NMT models are superior to non-attentional  ones  in  many  cases,  for  example  intranslating names and handling long sentences.AcknowledgmentWe  gratefully  acknowledge  support  from  a  giftfrom Bloomberg L.P. and the support of NVIDIA
Corporation with the donation of Tesla K40 GPUs.We  thank  Andrew  Ng  and  his  group  as  well  asthe  Stanford  Research  Computing  for  letting  ususe  their  computing  resources.    We  thank  Rus-sell Stewart for helpful discussions on the models.Lastly,  we thank  Quoc  Le,  Ilya  Sutskever,  OriolVinyals,  Richard  Socher,  Michael  Kayser,  JiweiLi,  Panupong  Pasupat,  Kelvin  Guu,  members  ofthe Stanford NLP Group and the annonymous re-viewers for their valuable comments and feedback.References[Bahdanau et al.2015]  D.   Bahdanau,    K.   Cho,    andY.  Bengio.   2015.   Neural  machine  translation  byjointly learning to align and translate. InICLR.[Buck et al.2014]  Christian  Buck,   Kenneth  Heafield,and Bas van Ooyen.  2014.  N-gram counts and lan-guage models from the common crawl. InLREC.[Cho et al.2014]  Kyunghyun  Cho,  Bart  van  Merrien-boer,   Caglar  Gulcehre,   Fethi  Bougares,   HolgerSchwenk,  and  Yoshua  Bengio.2014.Learningphrase representations using RNN encoder-decoderfor statistical machine translation. InEMNLP.[Fraser and Marcu2007]  Alexander  Fraser  and  DanielMarcu.   2007.   Measuring word alignment qualityfor  statistical  machine  translation.ComputationalLinguistics, 33(3):293–303.[Gregor et al.2015]  Karol Gregor, Ivo Danihelka, AlexGraves,  Danilo Jimenez Rezende,  and Daan Wier-stra.  2015.  DRAW: A recurrent neural network forimage generation. InICML.[Jean et al.2015]  S ́ebastien   Jean,Kyunghyun   Cho,Roland Memisevic, and Yoshua Bengio.  2015.  Onusing  very  large  target  vocabulary  for  neural  ma-chine translation. InACL.[Kalchbrenner and Blunsom2013]  N. Kalchbrenner andP. Blunsom. 2013. Recurrent continuous translationmodels. InEMNLP.[Koehn et al.2003]  Philipp  Koehn,   Franz  Josef  Och,and Daniel Marcu.   2003.   Statistical phrase-basedtranslation. InNAACL.[Liang et al.2006]  P.  Liang,  B.  Taskar,  and  D.  Klein.2006. Alignment by agreement. InNAACL.[Luong et al.2015]  M.-T. Luong, I. Sutskever, Q. V. Le,O. Vinyals, and W. Zaremba.  2015.  Addressing therare word problem in neural machine translation. InACL.[Mnih et al.2014]  Volodymyr   Mnih,   Nicolas   Heess,Alex Graves, and Koray Kavukcuoglu.  2014.  Re-current models of visual attention. InNIPS.[Papineni et al.2002]  Kishore Papineni, Salim Roukos,Todd  Ward,  and  Wei  jing  Zhu.2002.    Bleu:   amethod for automatic evaluation of machine trans-lation. InACL.[Sutskever et al.2014]  I.   Sutskever,   O.   Vinyals,   andQ. V. Le. 2014. Sequence to sequence learning withneural networks. InNIPS.[Xu et al.2015]  Kelvin  Xu,   Jimmy  Ba,   Ryan  Kiros,Kyunghyun   Cho,   Aaron   C.   Courville,   RuslanSalakhutdinov, Richard S. Zemel, and Yoshua Ben-gio. 2015. Show, attend and tell: Neural image cap-tion generation with visual attention. InICML.[Zaremba et al.2015]  WojciechZaremba,IlyaSutskever,  and  Oriol  Vinyals.2015.Recurrentneural network regularization. InICLR.A    Alignment VisualizationWe visualize  the alignment weights produced  byour different attention models in Figure 7. The vi-sualization  of  the  local  attention  model  is  muchsharper than that of the global one.  This contrastmatches our expectation that local attention is de-signed  to  only  focus  on  a  subset  of  words  eachtime. Also, since we translate from English to Ger-man and reverse the source English sentence, thewhite strides at the words“reality”and“.”in theglobal  attention  model  reveals  an  interesting  ac-cess pattern: it tends to refer back to the beginningof the source sequence.Compared  to  the  alignment  visualizations  in(Bahdanau et al., 2015),   our  alignment   patternsare not as sharp as theirs.  Such difference  couldpossibly  be  due  to  the  fact  that  translating  fromEnglish to German is harder than translating intoFrench as done in (Bahdanau et al., 2015), whichis an interesting point to examine in future work.
TheydonotunderstandwhyEuropeexistsintheorybutnotinreality.Sieverstehennicht,warumEuropatheoretischzwarexistiert,abernichtinWirklichkeit.TheydonotunderstandwhyEuro