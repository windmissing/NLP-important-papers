论文地址: https://arxiv.org/pdf/1706.03762.pdf  

Transformer是一种不同于CNN或RNN的新的网络结构。在Transformer之前，序列模型或序列转换问题普遍都是用基于gate和recurrent的网络结构。
本文用Attention代替了recurrent结构，在“并行性差”的问题是缓解，解决了“长距离依赖关系难以学习”的问题。  
